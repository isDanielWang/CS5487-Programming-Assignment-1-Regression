{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Shixiang WANG\n",
    "\n",
    "**EID:** sxwang6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5487: Programming Assignment 1 Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Polynomial function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"images/1/b/1-b.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different methods' mean square error can be seen as following table.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Method  | Mean Square Error |\n",
    "|  ----  | ----  |\n",
    "| least-squares (LS) | 0.40864388356987896 |\n",
    "| regularized LS (RLS), lambda = 0.1 | 0.4082365182026743 |\n",
    "| L1-regularized LS (LASSO), lambda = 1.0  | 0.47460685881919035 |\n",
    "| Bayesian regression (BR), alpha = 1.0, sigmoid^2 = 5 | 0.4591579387529853 |\n",
    "| robust regression (RR)  | 0.7680461505133539 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here I run 15 trials of each training subset of each model. Results can be seen as follows. The ratio I choose is [0.15, 0.25, 0.5, 0.75, 1.0]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**least-squares (LS)**\n",
    "<br>\n",
    "<img  src=\"images/1/c/LS/LS.png\">\n",
    "<br>\n",
    "**regularized LS (RLS)**\n",
    "<br>\n",
    "<img  src=\"images/1/c/RLS/RLS.png\">\n",
    "<br>\n",
    "**L1-regularized LS (LASSO)**\n",
    "<br>\n",
    "<img  src=\"images/1/c/LASSO/LASSO.png\">\n",
    "<br>\n",
    "**Bayesian regression (BR)**\n",
    "<br>\n",
    "<img  src=\"images/1/c/BR/BR.png\">\n",
    "<br>\n",
    "**robust regression (RR)**\n",
    "<br>\n",
    "<img  src=\"images/1/c/RR/RR.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Size vs. Mean square Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"images/1/c/Train_size_vs_MSE.png\" width = '500' height = '400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Method\\Ratio | 0.15 | 0.25 | 0.5 | 0.75 | 1.0 |\n",
    "|  ----  | ----  | ----  | ----  | ----  | ----  |\n",
    "| least-squares (LS) | 5.90464251 | 58.28685765 | 92.54639227 | 1.11137385 | 0.40864388 |\n",
    "| regularized LS (RLS), lambda = 0.1 | 27.40219333 | 21.95650343 | 1.84975453 | 0.84553814 | 0.40823652 |\n",
    "| L1-regularized LS (LASSO), lambda = 1.0  | 17.70630888 | 1.61668217 | 0.86909614 | 0.52370164 | 0.47460686 |\n",
    "| Bayesian regression (BR), alpha = 1.0, sigmoid^2 = 5 | 10.35716102 | 0.8670271 |  0.85655586 | 0.48468392 | 0.45915794 |\n",
    "| robust regression (RR)  | 86.11688626 | 5.5725608 | 1.29050328 | 0.87734935 | 0.76804615 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Important trends and findings\n",
    "    - With the increasing amount of training data, all five methods' MSE decreases.\n",
    "    - If we use all train data (the ratio is 1.0), then shuffling the training set does not affect the outcome (In question b, I set the shuffle false. But in question c, I set it true. When the ratio is 1.0, MSE in question b equals to it in question c).\n",
    "    - If the training data is enough, all five methods could get good results and low MSE.\n",
    "    - Least-square and Regularized Ls are more sensitive to the amount of training set than the other three models. From the 'Train size vs. Mean square error,' we could find that the least-square's curve is unstable and goes to low MSE slowest. Regularized's curve is a little more stable than the least-squares, but it still needs more data to get a low MSE than the other three methods.\n",
    "    - Bayesian regression performs better than the other four models, especially when training data is small. This is because we give the parameter an good prior distribution. When the amount of training data is small, we use the prior!\n",
    "    - Regularized LS performs better than least-squares no matter how much training data we use. Since We apply the penalty to parameter theta to prevent overfitting when training data is small. \n",
    "    - When train data are enough, robust regression has the highest MSE. This is because the complexity of the model does not match the amount of training data. So robust regression will have a little higher bias and variance than the other four models when train data is a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"images/1/d/outliers.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Method  | Mean Square Error|\n",
    "|  ----  | ----  |\n",
    "| least-squares (LS) | 31.866611893180647 |\n",
    "| regularized LS (RLS), lambda = 0.1 | 45.31939833090015 |\n",
    "| L1-regularized LS (LASSO), lambda = 1.0  | 14.350986453892133 |\n",
    "| Bayesian regression (BR), alpha = 1.0, sigmoid^2 = 5 | 12.728263707861979 |\n",
    "| robust regression (RR)  | 1.0077151523466126 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LASSO, Bayesian regression and robust regression are more robust to outliers.\n",
    "    - LASSO uses the L1 norm to regularize the parameter theta. L1 norm pushes parameters to be zero, which helps the model to be robust. The prior distribution of LASSO is the Laplace distribution, which also shows that LASSO will push weights to be zero.\n",
    "    - Bayesian regression. From the lecture, we know that mean of the estimated parameter theta is the linear combination of the maximum likelihood estimate and the prior. After adding outliers, if the variance of the training data is very large, the mean of theta will be closed to our prior. In other words, our prior is a type of regularization, which improves the model's robustness.\n",
    "    - Robust regression. As robust regression uses absolute to calculate the loss, its loss will be smaller than the method using the square of the difference to measure the loss when the difference between observation and prediction is larger than 1. This feature makes robust regression robust to outliers.\n",
    "- Least-squares and regularized LS are not robust to outliers.\n",
    "     - Least-square using squared error, when the difference is large, the error will be the square of the difference. So it is not robust to outliers.\n",
    "     - Although regularized LS has a regularized term, the regularized term is still a squared term that only focuses on the large weights. When there are many outliers, their weights will still not be zero. This causes regularized LS not to be robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here I tried the 10th order. The following figures compared models using 5th order and 10th order.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"images/1/e/e.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Method  | Mean Square Error|\n",
    "|  ----  | ----  |\n",
    "| least-squares (LS) | 7.983106594077762 |\n",
    "| regularized LS (RLS), lambda = 0.1 | 17.82521114379857 |\n",
    "| L1-regularized LS (LASSO), lambda = 1.0  | 6.142714741765921 |\n",
    "| Bayesian regression (BR), alpha = 1.0, sigmoid^2 = 5 | 3.0432536006051407 |\n",
    "| robust regression (RR)  | 2.7680461505133539 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Least-square,  regularized LS and LASSO tend to overfit the data.\n",
    "    - All these three models use squared error to measure the loss. If the input feature becomes very complex, these models' complexity will be higher than robust regression. So they have more chance to overfit the data.\n",
    "- Bayesian regression and robust regression do not overfit the data.\n",
    "    - Bayesian regression: If the variance of maximum likelihood estimate is very large, for example, the model is overfitting, mu of Bayesian regression will tend to prior. We give a pretty good prior distribution to our model, which helps the model prevent overfitting.\n",
    "    - Robust regression has lower model complexity than the other four models. So improving the complexity of the input feature will have less effect on it than on the other four models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 A real world regression problem – counting people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"images/2/a/a.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Method  | Mean Square Error | Mean Absoulte Error |\n",
    "|  ----  | ----  | ----  |\n",
    "| least-squares (LS) | 3.1391479166666683 | 1.333775 |\n",
    "| regularized LS (RLS), lambda = 0.1 | 3.0000145833333347 | 1.3229 |\n",
    "| L1-regularized LS (LASSO), lambda = 1.0  | 2.8160562500000013 | 1.2850916666666667 |\n",
    "| Bayesian regression (BR), alpha = 1.0, sigmoid^2 = 5 | 3.3126895833333343 | 1.4670416666666668 |\n",
    "| robust regression (RR)  | 3.1821145833333344 | 1.355175 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All five models work pretty well in this case. They all have small MSE and MAE. Among them, LASSO works best as it has the lowest MAE and MSE.\n",
    "- All five models make a bad prediction in around [x300, x400].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here I tried the 2nd order polynomial as φ(x) = [ x1, x2, x3, x4, x5, x6, x7, x8, x9, x1^2, x2^2, x3^2, x4^2, x5^2, x6^2, x7^2, x8^2, x9^2]'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"images/2/b/b.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Method  | Mean Square Error | Mean Absoulte Error |\n",
    "|  ----  | ----  | ----  |\n",
    "| least-squares (LS) | 3.043697916666668 | 1.3206166666666668 |\n",
    "| regularized LS (RLS), lambda = 0.1 | 2.8019145833333345 | 1.2723666666666669 |\n",
    "| L1-regularized LS (LASSO), lambda = 1.0  | 2.6504979166666676 | 1.2337666666666667 |\n",
    "| Bayesian regression (BR), alpha = 1.0, sigmoid^2 = 5 | 3.0701229166666666 | 1.3075333333333332 |\n",
    "| robust regression (RR)  | 3.017197916666668 | 1.3114916666666667 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The results are improved but not a lot. All five models have smaller MSE and MAE than using original training data. Doing non-linear transformation of the input features could give our models more information about the data then the original feature. And this extra information helps our model better fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[1] https://numpy.org/doc/stable/reference/routines.linalg.html\n",
    "<br>\n",
    "[2] https://cvxopt.org/examples/#tutorial-examples"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fddfbec118b8a6eec00f8005456499021538369a6963e34d464306f8d055c8f2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
